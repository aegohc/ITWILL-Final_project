# -*- coding: utf-8 -*-

"""
Chap02 - TextVectorizing_RNN
lecture01_vectorizing_encoding

- 희소행렬 + DNN model
<작업절차>
1. csv file load
2. texts와 label 전처리
3. num_words = 4000 제한
4. Sparse matrix : features 
5. train/ test split
6. DNN model
"""


# text 처리
import pandas as pd # csv file
import numpy as np  # list -> numpy
import string       # texts 전처리

from tensorflow.keras.preprocessing.text import Tokenizer         # 토큰 생성기
from tensorflow.keras.preprocessing.sequence import pad_sequences # 패딩 : 일정한 길이의 벡터생성
from tensorflow.keras.utils import to_categorical                 # 인코딩
from sklearn.model_selection import train_test_split              # split
import time # time check

# DNN model 생성
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Flatten

# 파일 읽어오기 
# 1. csv file load
path = 'K:/ITWILL/Final_project/'
minwon_data = pd.read_csv(path + 'minwon_crawling4400.csv', header = None)
minwon_data.info()
'''
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 4401 entries, 0 to 4400
Data columns (total 3 columns):
 #   Column  Non-Null Count  Dtype 
---  ------  --------------  ----- 
 0   0       4401 non-null   object
 1   1       1 non-null      object
 2   2       4401 non-null   object
'''

titles = minwon_data[0]
contents = minwon_data[1]
replies = minwon_data[2]

print(titles)

# 2. titles, contents, replies 전처리
# 0) titles 전처리
# 음.. 예제코드는 0,1으로 스팸/비스팸 구분했는데 우리는 어떤식으로 진행

# 1) titles 전처리 -> [text_sample.txt] 참고
def text_prepro(titles):
    # Lower case : 소문자
    titles = [x.lower() for x in titles]
    # Remove punctuation : 문장부호 제거
    titles = [''.join(c for c in x if c not in string.punctuation) for x in titles]
    # Remove numbers : 숫자 제거
    titles = [''.join(c for c in x if c not in string.digits) for x in titles]
    # Trim extra whitespace : 공백 제거
    titles = [' '.join(x.split()) for x in titles]
    return titles

# 함수 호출
titles = text_prepro(titles)
print(titles)


# 3. num_words = 1000개 제한()
tokenizer = Tokenizer()                  # 1차 : 전체 단어 이용
#tokenizer = Tokenizer(num_words = 1000) # 2차 : 1000개의 단어 이용 -> 희소행렬에 영향

tit = tokenizer.fit_on_texts(titles)    # title 반영 -> token 생성
rep = tokenizer.fit_on_texts(replies)   # replies 반영 -> token 생성

tit_token = tokenizer.index_word        # title 토큰 반환  
rep_token = tokenizer.index_word        # replies 토큰 반환
# print(token) # {'word':고유숫자}
print(tit_token)  # 1596
print(rep_token)  # 4711

# 전체 단어수 +1
title_size = len(tit_token)+1
reply_size = len(rep_token)+1

# 4. Sparse matrix : [docs, terms] features
tit_data = tokenizer.texts_to_matrix(texts=titles, mode='tfidf') # tfidf : 가장선호하는 방식(예측력짱)
tit_data.shape # 1차: (4401, 1597) 1598 = 1597+1 = 전체단어길이+1

rep_data = tokenizer.texts_to_matrix(texts=replies, mode='tfidf') # tfidf : 가장선호하는 방식(예측력짱)
rep_data.shape  # (4401, 4712) 4713 = 4712+1


# 5. train/ test split : 80% vs 20% (제목으로 분류하기)
x_train, x_val, y_train, y_val = train_test_split(tit_data, sep, test_size=0.2)

type(x_train) # numpy.ndarray
type(y_train) # numpy.ndarray

# 6. DNN model
model = Sequential() # keras model

input_shape = (1598,)

# hidden layer1 : w[1598,64]
model.add(Dense(units=64, input_shape=input_shape, activation = 'relu'))

# hidden layer : w[64,32]
model.add(Dense(units=32, activation = 'relu'))

# output layer : w[32,1]
model.add(Dense(units=1, activation='sigmoid'))


model.summary()
'''
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 64)                102336    
_________________________________________________________________
dense_1 (Dense)              (None, 32)                2080      
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 33        
=================================================================
'''

# 7. model compile : 학습과정 설정(이항분류기)
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# 8. 학습 model training : train(80%) vs val(20%)
X = x_train.astype(float)
Y = sep.astype(float)
sep = float(sep)
model.fit(X, y_train,
          epochs=5,
          batch_size=512,
          verbose=1,
          validation_data=(x_val, y_val))


'''
#############################################
### 코사인 유사도
#############################################
from sklearn.feature_extraction.text import TfidfVectorizer

# 객체 생성
tfidf_vectorizer = TfidfVectorizer()

# 문장 벡터화 진행
tfidf_matrix = tfidf_vectorizer.fit_transform(titles)
#tfidf_matrix = tfidf_vectorizer.fit_transform(replies)

# 각 단어
tit_word = tfidf_vectorizer.get_feature_names()
rep_word = tfidf_vectorizer.get_feature_names()

# 각 단어의 벡터 값
tit_idf = tfidf_vectorizer.idf_
rep_idf = tfidf_vectorizer.idf_

print(tit_idf)
len(tit_idf) #  1551

print(rep_idf)
len(rep_idf) # 4451

print(dict(zip(tit_word, tit_idf)))

# 코사인 유사도
from sklearn.metrics.pairwise import cosine_similarity
cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])
'''































