# -*- coding: utf-8 -*-

"""
Chap02 - TextVectorizing_RNN
lecture01_vectorizing_encoding

- 희소행렬 + DNN model
<작업절차>
1. csv file load
2. texts와 label 전처리
3. num_words = 4000 제한
4. Sparse matrix : features 
5. train/ test split
6. DNN model
"""


# text 처리
import pandas as pd # csv file
import numpy as np  # list -> numpy
import string       # texts 전처리

from tensorflow.keras.preprocessing.text import Tokenizer         # 토큰 생성기
from tensorflow.keras.preprocessing.sequence import pad_sequences # 패딩 : 일정한 길이의 벡터생성
from tensorflow.keras.utils import to_categorical                 # 인코딩
from sklearn.model_selection import train_test_split              # split
import time # time check

# DNN model 생성
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Flatten

# 파일 읽어오기 
# 1. csv file load
path = 'K:/ITWILL/Final_project/'
minwon_data = pd.read_csv(path + 'minwon_crawling4400.csv', header = None)
minwon_data.info()
'''
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 4401 entries, 0 to 4400
Data columns (total 3 columns):
 #   Column  Non-Null Count  Dtype 
---  ------  --------------  ----- 
 0   0       4401 non-null   object
 1   1       1 non-null      object
 2   2       4401 non-null   object
'''

titles = minwon_data[0]
contents = minwon_data[1]
replies = minwon_data[2]

print(titles)

# 2. titles, contents, replies 전처리
# 0) titles 전처리
# 음.. 예제코드는 0,1으로 스팸/비스팸 구분했는데 우리는 어떤식으로 진행

# 1) titles 전처리 -> [text_sample.txt] 참고
def text_prepro(titles):
    # Lower case : 소문자
    titles = [x.lower() for x in titles]
    # Remove punctuation : 문장부호 제거
    titles = [''.join(c for c in x if c not in string.punctuation) for x in titles]
    # Remove numbers : 숫자 제거
    titles = [''.join(c for c in x if c not in string.digits) for x in titles]
    # Trim extra whitespace : 공백 제거
    titles = [' '.join(x.split()) for x in titles]
    return titles

# 함수 호출
titles = text_prepro(titles)
print(titles)


# 3. num_words = 1000개 제한()
tokenizer = Tokenizer()                 # 1차 : 전체 단어 이용
tokenizer = Tokenizer(num_words = 1000) # 2차 : 1000개의 단어 이용 -> 희소행렬에 영향
tokenizer.fit_on_texts(titles)          # 텍스트 반영 -> token 생성
token = tokenizer.word_index            # 토큰 반환  

print(token) # {'word':고유숫자}






























