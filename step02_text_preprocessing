# -*- coding: utf-8 -*-

"""
Chap02 - TextVectorizing_RNN
lecture01_vectorizing_encoding

- 희소행렬 + DNN model
<작업절차>
1. csv file load
2. texts와 label 전처리
3. num_words = 4000 제한
4. Sparse matrix : features 
5. train/ test split
6. DNN model
"""


# text 처리
import pandas as pd # csv file
import numpy as np  # list -> numpy
import string       # texts 전처리

from sklearn.model_selection import train_test_split              # split
from tensorflow.keras.preprocessing.text import Tokenizer         # 토큰 생성기
from tensorflow.keras.preprocessing.sequence import pad_sequences # 패딩 : 일정한 길이의 벡터생성
from tensorflow.keras.utils import to_categorical                 # 인코딩
import time # time check

# DNN model 생성
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, Flatten


# 파일 읽어오기 
# 1. csv file load
path = 'K:/ITWILL/Final_project/'
minwon_data = pd.read_csv(path + 'minwon_crawling4400.csv')
minwon_data.info()
'''
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 4400 entries, 0 to 4399
Data columns (total 4 columns):
 #   Column    Non-Null Count  Dtype  
---  ------    --------------  -----  
 0   titles    4400 non-null   object 
 1   contents  0 non-null      float64
 2   replies   4400 non-null   object 
 3   sep       4399 non-null   float64
'''

titles = minwon_data['titles']
sep = minwon_data['sep']


# 2. titles, contents, replies 전처리

# 1) titles 전처리 -> [text_sample.txt] 참고
def title_prepro(titles):
    # Lower case : 소문자
    titles = [x.lower() for x in titles]
    # Remove punctuation : 문장부호 제거
    titles = [''.join(c for c in x if c not in string.punctuation) for x in titles]
    # Remove numbers : 숫자 제거
    titles = [''.join(c for c in x if c not in string.digits) for x in titles]
    # Trim extra whitespace : 공백 제거
    titles = [' '.join(x.split()) for x in titles]
    return titles

# 함수 호출
titles = title_prepro(titles)
print(titles)


# 3. num_words = 1000개 제한()
tokenizer = Tokenizer()                  # 1차 : 전체 단어 이용
tit = tokenizer.fit_on_texts(titles)    # title 반영 -> token 생성
tit_token = tokenizer.index_word        # title 토큰 반환  
print(tit_token)  # 1591

# 전체 단어수 +1
title_size = len(tit_token) + 1
#reply_size = len(rep_token)+1


# 4. sequence (정수 색인)
seq_result = tokenizer.texts_to_sequences(titles)
print(seq_result) # 고유번호를 출력 -> [] 하나의 문장, 숫자 : 하나의 고유단어

lens = [len(sent) for sent in seq_result]
print(lens)
maxlen = max(lens)
print(maxlen) # 30

# 4-1. padding : maxlen기준으로 단어 길이 맞춤
tit_data= pad_sequences(seq_result, maxlen=maxlen)
tit_data.shape # (4399, 30) - (문장, 단어길이)


# 5. train/ test split : 80% vs 20% (제목으로 분류하기)
x_train, x_val, y_train, y_val = train_test_split(tit_data, sep, test_size=0.2)


# 6. DNN model
model = Sequential() # keras model

# Embedding layer - 1층
model.add(Embedding(input_dim = title_size, output_dim =32, input_length=maxlen))

# 2d -> 1d : Flatten layer -> Embedding방식으로 인코딩한 후 DNN모델과 결합
model.add(Flatten())

input_shape = (1598,)
# hidden layer1 : w[1598,64]
model.add(Dense(units=64, input_shape=input_shape, activation = 'relu'))

# hidden layer : w[64,32]
model.add(Dense(units=32, activation = 'relu'))

# output layer : w[32,1]
model.add(Dense(units=1, activation='sigmoid'))

model.summary()


# 7. model compile : 학습과정 설정(이항분류기)
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# 8. 학습 model training : train(80%) vs val(20%)
model.fit(X, y_train,
          epochs=5,
          batch_size=512,
          verbose=1,
          validation_data=(x_val, y_val))

# 9. model evaluation : val dataset
print('model evaluation')
model.evaluate(x = x_val, y = y_val)










'''
#############################################
### 코사인 유사도
#############################################
from sklearn.feature_extraction.text import TfidfVectorizer

# 객체 생성
tfidf_vectorizer = TfidfVectorizer()

# 문장 벡터화 진행
tfidf_matrix = tfidf_vectorizer.fit_transform(titles)
#tfidf_matrix = tfidf_vectorizer.fit_transform(replies)

# 각 단어
tit_word = tfidf_vectorizer.get_feature_names()
rep_word = tfidf_vectorizer.get_feature_names()

# 각 단어의 벡터 값
tit_idf = tfidf_vectorizer.idf_
rep_idf = tfidf_vectorizer.idf_

print(tit_idf)
len(tit_idf) #  1551

print(rep_idf)
len(rep_idf) # 4451

print(dict(zip(tit_word, tit_idf)))

# 코사인 유사도
from sklearn.metrics.pairwise import cosine_similarity
cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])
'''



